{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de276e5d-2818-47de-b662-62e857d2e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23215"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q \"openvino-dev>=2024.0.0\"\n",
    "%pip install -q tensorflow\n",
    "%pip install -q opencv-python requests tqdm\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4f90f5-2568-4f09-949b-e306e95fc048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23215"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import tarfile\n",
    "import time, os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import openvino as ov\n",
    "from openvino.tools.mo.front import tf as ov_tf_front\n",
    "from openvino.tools import mo\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "import notebook_utils as utils\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e1c4d2-3173-4b9f-9e35-c9a9cd073288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data', ['Mconv7_stage2_L1', 'Mconv7_stage2_L2'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########detection\n",
    "base_model_dir = Path(\"model\")\n",
    "\n",
    "# The name of the model from Open Model Zoo\n",
    "model_name = \"ssdlite_mobilenet_v2\"\n",
    "\n",
    "archive_name = Path(f\"{model_name}_coco_2018_05_09.tar.gz\")\n",
    "model_url = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/public/2022.1/{model_name}/{archive_name}\"\n",
    "\n",
    "# Download the archive\n",
    "downloaded_model_path = base_model_dir / archive_name\n",
    "if not downloaded_model_path.exists():\n",
    "    utils.download_file(model_url, downloaded_model_path.name, downloaded_model_path.parent)\n",
    "\n",
    "# Unpack the model\n",
    "tf_model_path = base_model_dir / archive_name.with_suffix(\"\").stem / \"frozen_inference_graph.pb\"\n",
    "if not tf_model_path.exists():\n",
    "    with tarfile.open(downloaded_model_path) as file:\n",
    "        file.extractall(base_model_dir)\n",
    "\n",
    "precision = \"FP16\"\n",
    "# The output path for the conversion.\n",
    "converted_model_path = Path(\"model\") / f\"{model_name}_{precision.lower()}.xml\"\n",
    "\n",
    "# Convert it to IR if not previously converted\n",
    "trans_config_path = Path(ov_tf_front.__file__).parent / \"ssd_v2_support.json\"\n",
    "if not converted_model_path.exists():\n",
    "    ov_model = mo.convert_model(\n",
    "        tf_model_path,\n",
    "        compress_to_fp16=(precision == \"FP16\"),\n",
    "        transformations_config=trans_config_path,\n",
    "        tensorflow_object_detection_api_pipeline_config=tf_model_path.parent / \"pipeline.config\",\n",
    "        reverse_input_channels=True,\n",
    "    )\n",
    "    ov.save_model(ov_model, converted_model_path)\n",
    "    del ov_model\n",
    "###########\n",
    "\n",
    "###########pose estimation\n",
    "# The name of the model from Open Model Zoo.\n",
    "model_name2 = \"human-pose-estimation-0001\"\n",
    "# Selected precision (FP32, FP16, FP16-INT8).\n",
    "precision2 = \"FP16-INT8\"\n",
    "\n",
    "model_path2 = base_model_dir / \"intel\" / model_name2 / precision2 / f\"{model_name2}.xml\"\n",
    "\n",
    "if not model_path2.exists():\n",
    "    model_url_dir2 = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/3/{model_name2}/{precision2}/\"\n",
    "    utils.download_file(model_url_dir + model_name2 + \".xml\", model_path2.name, model_path2.parent)\n",
    "    utils.download_file(\n",
    "        model_url_dir2 + model_name2 + \".bin\",\n",
    "        model_path2.with_suffix(\".bin\").name,\n",
    "        model_path2.parent,\n",
    "    )\n",
    "############\n",
    "\n",
    "# code from https://github.com/openvinotoolkit/open_model_zoo/blob/9296a3712069e688fe64ea02367466122c8e8a3b/demos/common/python/models/open_pose.py#L135\n",
    "class OpenPoseDecoder:\n",
    "    BODY_PARTS_KPT_IDS = (\n",
    "        (1, 2),\n",
    "        (1, 5),\n",
    "        (2, 3),\n",
    "        (3, 4),\n",
    "        (5, 6),\n",
    "        (6, 7),\n",
    "        (1, 8),\n",
    "        (8, 9),\n",
    "        (9, 10),\n",
    "        (1, 11),\n",
    "        (11, 12),\n",
    "        (12, 13),\n",
    "        (1, 0),\n",
    "        (0, 14),\n",
    "        (14, 16),\n",
    "        (0, 15),\n",
    "        (15, 17),\n",
    "        (2, 16),\n",
    "        (5, 17),\n",
    "    )\n",
    "    BODY_PARTS_PAF_IDS = (\n",
    "        12,\n",
    "        20,\n",
    "        14,\n",
    "        16,\n",
    "        22,\n",
    "        24,\n",
    "        0,\n",
    "        2,\n",
    "        4,\n",
    "        6,\n",
    "        8,\n",
    "        10,\n",
    "        28,\n",
    "        30,\n",
    "        34,\n",
    "        32,\n",
    "        36,\n",
    "        18,\n",
    "        26,\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_joints=18,\n",
    "        skeleton=BODY_PARTS_KPT_IDS,\n",
    "        paf_indices=BODY_PARTS_PAF_IDS,\n",
    "        max_points=100,\n",
    "        score_threshold=0.1,\n",
    "        min_paf_alignment_score=0.05,\n",
    "        delta=0.5,\n",
    "    ):\n",
    "        self.num_joints = num_joints\n",
    "        self.skeleton = skeleton\n",
    "        self.paf_indices = paf_indices\n",
    "        self.max_points = max_points\n",
    "        self.score_threshold = score_threshold\n",
    "        self.min_paf_alignment_score = min_paf_alignment_score\n",
    "        self.delta = delta\n",
    "\n",
    "        self.points_per_limb = 10\n",
    "        self.grid = np.arange(self.points_per_limb, dtype=np.float32).reshape(1, -1, 1)\n",
    "\n",
    "    def __call__(self, heatmaps, nms_heatmaps, pafs):\n",
    "        batch_size, _, h, w = heatmaps.shape\n",
    "        assert batch_size == 1, \"Batch size of 1 only supported\"\n",
    "\n",
    "        keypoints = self.extract_points(heatmaps, nms_heatmaps)\n",
    "        pafs = np.transpose(pafs, (0, 2, 3, 1))\n",
    "\n",
    "        if self.delta > 0:\n",
    "            for kpts in keypoints:\n",
    "                kpts[:, :2] += self.delta\n",
    "                np.clip(kpts[:, 0], 0, w - 1, out=kpts[:, 0])\n",
    "                np.clip(kpts[:, 1], 0, h - 1, out=kpts[:, 1])\n",
    "\n",
    "        pose_entries, keypoints = self.group_keypoints(keypoints, pafs, pose_entry_size=self.num_joints + 2)\n",
    "        poses, scores = self.convert_to_coco_format(pose_entries, keypoints)\n",
    "        if len(poses) > 0:\n",
    "            poses = np.asarray(poses, dtype=np.float32)\n",
    "            poses = poses.reshape((poses.shape[0], -1, 3))\n",
    "        else:\n",
    "            poses = np.empty((0, 17, 3), dtype=np.float32)\n",
    "            scores = np.empty(0, dtype=np.float32)\n",
    "\n",
    "        return poses, scores\n",
    "\n",
    "    def extract_points(self, heatmaps, nms_heatmaps):\n",
    "        batch_size, channels_num, h, w = heatmaps.shape\n",
    "        assert batch_size == 1, \"Batch size of 1 only supported\"\n",
    "        assert channels_num >= self.num_joints\n",
    "\n",
    "        xs, ys, scores = self.top_k(nms_heatmaps)\n",
    "        masks = scores > self.score_threshold\n",
    "        all_keypoints = []\n",
    "        keypoint_id = 0\n",
    "        for k in range(self.num_joints):\n",
    "            # Filter low-score points.\n",
    "            mask = masks[0, k]\n",
    "            x = xs[0, k][mask].ravel()\n",
    "            y = ys[0, k][mask].ravel()\n",
    "            score = scores[0, k][mask].ravel()\n",
    "            n = len(x)\n",
    "            if n == 0:\n",
    "                all_keypoints.append(np.empty((0, 4), dtype=np.float32))\n",
    "                continue\n",
    "            # Apply quarter offset to improve localization accuracy.\n",
    "            x, y = self.refine(heatmaps[0, k], x, y)\n",
    "            np.clip(x, 0, w - 1, out=x)\n",
    "            np.clip(y, 0, h - 1, out=y)\n",
    "            # Pack resulting points.\n",
    "            keypoints = np.empty((n, 4), dtype=np.float32)\n",
    "            keypoints[:, 0] = x\n",
    "            keypoints[:, 1] = y\n",
    "            keypoints[:, 2] = score\n",
    "            keypoints[:, 3] = np.arange(keypoint_id, keypoint_id + n)\n",
    "            keypoint_id += n\n",
    "            all_keypoints.append(keypoints)\n",
    "        return all_keypoints\n",
    "\n",
    "    def top_k(self, heatmaps):\n",
    "        N, K, _, W = heatmaps.shape\n",
    "        heatmaps = heatmaps.reshape(N, K, -1)\n",
    "        # Get positions with top scores.\n",
    "        ind = heatmaps.argpartition(-self.max_points, axis=2)[:, :, -self.max_points :]\n",
    "        scores = np.take_along_axis(heatmaps, ind, axis=2)\n",
    "        # Keep top scores sorted.\n",
    "        subind = np.argsort(-scores, axis=2)\n",
    "        ind = np.take_along_axis(ind, subind, axis=2)\n",
    "        scores = np.take_along_axis(scores, subind, axis=2)\n",
    "        y, x = np.divmod(ind, W)\n",
    "        return x, y, scores\n",
    "\n",
    "    @staticmethod\n",
    "    def refine(heatmap, x, y):\n",
    "        h, w = heatmap.shape[-2:]\n",
    "        valid = np.logical_and(np.logical_and(x > 0, x < w - 1), np.logical_and(y > 0, y < h - 1))\n",
    "        xx = x[valid]\n",
    "        yy = y[valid]\n",
    "        dx = np.sign(heatmap[yy, xx + 1] - heatmap[yy, xx - 1], dtype=np.float32) * 0.25\n",
    "        dy = np.sign(heatmap[yy + 1, xx] - heatmap[yy - 1, xx], dtype=np.float32) * 0.25\n",
    "        x = x.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "        x[valid] += dx\n",
    "        y[valid] += dy\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def is_disjoint(pose_a, pose_b):\n",
    "        pose_a = pose_a[:-2]\n",
    "        pose_b = pose_b[:-2]\n",
    "        return np.all(np.logical_or.reduce((pose_a == pose_b, pose_a < 0, pose_b < 0)))\n",
    "\n",
    "    def update_poses(\n",
    "        self,\n",
    "        kpt_a_id,\n",
    "        kpt_b_id,\n",
    "        all_keypoints,\n",
    "        connections,\n",
    "        pose_entries,\n",
    "        pose_entry_size,\n",
    "    ):\n",
    "        for connection in connections:\n",
    "            pose_a_idx = -1\n",
    "            pose_b_idx = -1\n",
    "            for j, pose in enumerate(pose_entries):\n",
    "                if pose[kpt_a_id] == connection[0]:\n",
    "                    pose_a_idx = j\n",
    "                if pose[kpt_b_id] == connection[1]:\n",
    "                    pose_b_idx = j\n",
    "            if pose_a_idx < 0 and pose_b_idx < 0:\n",
    "                # Create new pose entry.\n",
    "                pose_entry = np.full(pose_entry_size, -1, dtype=np.float32)\n",
    "                pose_entry[kpt_a_id] = connection[0]\n",
    "                pose_entry[kpt_b_id] = connection[1]\n",
    "                pose_entry[-1] = 2\n",
    "                pose_entry[-2] = np.sum(all_keypoints[connection[0:2], 2]) + connection[2]\n",
    "                pose_entries.append(pose_entry)\n",
    "            elif pose_a_idx >= 0 and pose_b_idx >= 0 and pose_a_idx != pose_b_idx:\n",
    "                # Merge two poses are disjoint merge them, otherwise ignore connection.\n",
    "                pose_a = pose_entries[pose_a_idx]\n",
    "                pose_b = pose_entries[pose_b_idx]\n",
    "                if self.is_disjoint(pose_a, pose_b):\n",
    "                    pose_a += pose_b\n",
    "                    pose_a[:-2] += 1\n",
    "                    pose_a[-2] += connection[2]\n",
    "                    del pose_entries[pose_b_idx]\n",
    "            elif pose_a_idx >= 0 and pose_b_idx >= 0:\n",
    "                # Adjust score of a pose.\n",
    "                pose_entries[pose_a_idx][-2] += connection[2]\n",
    "            elif pose_a_idx >= 0:\n",
    "                # Add a new limb into pose.\n",
    "                pose = pose_entries[pose_a_idx]\n",
    "                if pose[kpt_b_id] < 0:\n",
    "                    pose[-2] += all_keypoints[connection[1], 2]\n",
    "                pose[kpt_b_id] = connection[1]\n",
    "                pose[-2] += connection[2]\n",
    "                pose[-1] += 1\n",
    "            elif pose_b_idx >= 0:\n",
    "                # Add a new limb into pose.\n",
    "                pose = pose_entries[pose_b_idx]\n",
    "                if pose[kpt_a_id] < 0:\n",
    "                    pose[-2] += all_keypoints[connection[0], 2]\n",
    "                pose[kpt_a_id] = connection[0]\n",
    "                pose[-2] += connection[2]\n",
    "                pose[-1] += 1\n",
    "        return pose_entries\n",
    "\n",
    "    @staticmethod\n",
    "    def connections_nms(a_idx, b_idx, affinity_scores):\n",
    "        # From all retrieved connections that share starting/ending keypoints leave only the top-scoring ones.\n",
    "        order = affinity_scores.argsort()[::-1]\n",
    "        affinity_scores = affinity_scores[order]\n",
    "        a_idx = a_idx[order]\n",
    "        b_idx = b_idx[order]\n",
    "        idx = []\n",
    "        has_kpt_a = set()\n",
    "        has_kpt_b = set()\n",
    "        for t, (i, j) in enumerate(zip(a_idx, b_idx)):\n",
    "            if i not in has_kpt_a and j not in has_kpt_b:\n",
    "                idx.append(t)\n",
    "                has_kpt_a.add(i)\n",
    "                has_kpt_b.add(j)\n",
    "        idx = np.asarray(idx, dtype=np.int32)\n",
    "        return a_idx[idx], b_idx[idx], affinity_scores[idx]\n",
    "\n",
    "    def group_keypoints(self, all_keypoints_by_type, pafs, pose_entry_size=20):\n",
    "        all_keypoints = np.concatenate(all_keypoints_by_type, axis=0)\n",
    "        pose_entries = []\n",
    "        # For every limb.\n",
    "        for part_id, paf_channel in enumerate(self.paf_indices):\n",
    "            kpt_a_id, kpt_b_id = self.skeleton[part_id]\n",
    "            kpts_a = all_keypoints_by_type[kpt_a_id]\n",
    "            kpts_b = all_keypoints_by_type[kpt_b_id]\n",
    "            n = len(kpts_a)\n",
    "            m = len(kpts_b)\n",
    "            if n == 0 or m == 0:\n",
    "                continue\n",
    "\n",
    "            # Get vectors between all pairs of keypoints, i.e. candidate limb vectors.\n",
    "            a = kpts_a[:, :2]\n",
    "            a = np.broadcast_to(a[None], (m, n, 2))\n",
    "            b = kpts_b[:, :2]\n",
    "            vec_raw = (b[:, None, :] - a).reshape(-1, 1, 2)\n",
    "\n",
    "            # Sample points along every candidate limb vector.\n",
    "            steps = 1 / (self.points_per_limb - 1) * vec_raw\n",
    "            points = steps * self.grid + a.reshape(-1, 1, 2)\n",
    "            points = points.round().astype(dtype=np.int32)\n",
    "            x = points[..., 0].ravel()\n",
    "            y = points[..., 1].ravel()\n",
    "\n",
    "            # Compute affinity score between candidate limb vectors and part affinity field.\n",
    "            part_pafs = pafs[0, :, :, paf_channel : paf_channel + 2]\n",
    "            field = part_pafs[y, x].reshape(-1, self.points_per_limb, 2)\n",
    "            vec_norm = np.linalg.norm(vec_raw, ord=2, axis=-1, keepdims=True)\n",
    "            vec = vec_raw / (vec_norm + 1e-6)\n",
    "            affinity_scores = (field * vec).sum(-1).reshape(-1, self.points_per_limb)\n",
    "            valid_affinity_scores = affinity_scores > self.min_paf_alignment_score\n",
    "            valid_num = valid_affinity_scores.sum(1)\n",
    "            affinity_scores = (affinity_scores * valid_affinity_scores).sum(1) / (valid_num + 1e-6)\n",
    "            success_ratio = valid_num / self.points_per_limb\n",
    "\n",
    "            # Get a list of limbs according to the obtained affinity score.\n",
    "            valid_limbs = np.where(np.logical_and(affinity_scores > 0, success_ratio > 0.8))[0]\n",
    "            if len(valid_limbs) == 0:\n",
    "                continue\n",
    "            b_idx, a_idx = np.divmod(valid_limbs, n)\n",
    "            affinity_scores = affinity_scores[valid_limbs]\n",
    "\n",
    "            # Suppress incompatible connections.\n",
    "            a_idx, b_idx, affinity_scores = self.connections_nms(a_idx, b_idx, affinity_scores)\n",
    "            connections = list(\n",
    "                zip(\n",
    "                    kpts_a[a_idx, 3].astype(np.int32),\n",
    "                    kpts_b[b_idx, 3].astype(np.int32),\n",
    "                    affinity_scores,\n",
    "                )\n",
    "            )\n",
    "            if len(connections) == 0:\n",
    "                continue\n",
    "\n",
    "            # Update poses with new connections.\n",
    "            pose_entries = self.update_poses(\n",
    "                kpt_a_id,\n",
    "                kpt_b_id,\n",
    "                all_keypoints,\n",
    "                connections,\n",
    "                pose_entries,\n",
    "                pose_entry_size,\n",
    "            )\n",
    "\n",
    "        # Remove poses with not enough points.\n",
    "        pose_entries = np.asarray(pose_entries, dtype=np.float32).reshape(-1, pose_entry_size)\n",
    "        pose_entries = pose_entries[pose_entries[:, -1] >= 3]\n",
    "        return pose_entries, all_keypoints\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_coco_format(pose_entries, all_keypoints):\n",
    "        num_joints = 17\n",
    "        coco_keypoints = []\n",
    "        scores = []\n",
    "        for pose in pose_entries:\n",
    "            if len(pose) == 0:\n",
    "                continue\n",
    "            keypoints = np.zeros(num_joints * 3)\n",
    "            reorder_map = [0, -1, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n",
    "            person_score = pose[-2]\n",
    "            for keypoint_id, target_id in zip(pose[:-2], reorder_map):\n",
    "                if target_id < 0:\n",
    "                    continue\n",
    "                cx, cy, score = 0, 0, 0  # keypoint not found\n",
    "                if keypoint_id != -1:\n",
    "                    cx, cy, score = all_keypoints[int(keypoint_id), 0:3]\n",
    "                keypoints[target_id * 3 + 0] = cx\n",
    "                keypoints[target_id * 3 + 1] = cy\n",
    "                keypoints[target_id * 3 + 2] = score\n",
    "            coco_keypoints.append(keypoints)\n",
    "            scores.append(person_score * max(0, (pose[-1] - 1)))  # -1 for 'neck'\n",
    "        return np.asarray(coco_keypoints), np.asarray(scores)\n",
    "\n",
    "decoder = OpenPoseDecoder()\n",
    "\n",
    "# 2D pooling in numpy (from: https://stackoverflow.com/a/54966908/1624463)\n",
    "def pool2d(A, kernel_size, stride, padding, pool_mode=\"max\"):\n",
    "    \"\"\"\n",
    "    2D Pooling\n",
    "\n",
    "    Parameters:\n",
    "        A: input 2D array\n",
    "        kernel_size: int, the size of the window\n",
    "        stride: int, the stride of the window\n",
    "        padding: int, implicit zero paddings on both sides of the input\n",
    "        pool_mode: string, 'max' or 'avg'\n",
    "    \"\"\"\n",
    "    # Padding\n",
    "    A = np.pad(A, padding, mode=\"constant\")\n",
    "\n",
    "    # Window view of A\n",
    "    output_shape = (\n",
    "        (A.shape[0] - kernel_size) // stride + 1,\n",
    "        (A.shape[1] - kernel_size) // stride + 1,\n",
    "    )\n",
    "    kernel_size = (kernel_size, kernel_size)\n",
    "    A_w = as_strided(\n",
    "        A,\n",
    "        shape=output_shape + kernel_size,\n",
    "        strides=(stride * A.strides[0], stride * A.strides[1]) + A.strides,\n",
    "    )\n",
    "    A_w = A_w.reshape(-1, *kernel_size)\n",
    "\n",
    "    # Return the result of pooling.\n",
    "    if pool_mode == \"max\":\n",
    "        return A_w.max(axis=(1, 2)).reshape(output_shape)\n",
    "    elif pool_mode == \"avg\":\n",
    "        return A_w.mean(axis=(1, 2)).reshape(output_shape)\n",
    "\n",
    "\n",
    "# non maximum suppression\n",
    "def heatmap_nms(heatmaps, pooled_heatmaps):\n",
    "    return heatmaps * (heatmaps == pooled_heatmaps)\n",
    "\n",
    "\n",
    "# Get poses from results.\n",
    "def pose_process_results(img, pafs, heatmaps):\n",
    "    # This processing comes from\n",
    "    # https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/common/python/models/open_pose.py\n",
    "    pooled_heatmaps = np.array([[pool2d(h, kernel_size=3, stride=1, padding=1, pool_mode=\"max\") for h in heatmaps[0]]])\n",
    "    nms_heatmaps = heatmap_nms(heatmaps, pooled_heatmaps)\n",
    "\n",
    "    # Decode poses.\n",
    "    poses, scores = decoder(heatmaps, nms_heatmaps, pafs)\n",
    "    output_shape = list(compiled_model.output(index=0).partial_shape)\n",
    "    output_scale = (\n",
    "        img.shape[1] / output_shape[3].get_length(),\n",
    "        img.shape[0] / output_shape[2].get_length(),\n",
    "    )\n",
    "    # Multiply coordinates by a scaling factor.\n",
    "    poses[:, :, :2] *= output_scale\n",
    "    return poses, scores\n",
    "\n",
    "pose_colors = (\n",
    "    (255, 0, 0),\n",
    "    (255, 0, 255),\n",
    "    (170, 0, 255),\n",
    "    (255, 0, 85),\n",
    "    (255, 0, 170),\n",
    "    (85, 255, 0),\n",
    "    (255, 170, 0),\n",
    "    (0, 255, 0),\n",
    "    (255, 255, 0),\n",
    "    (0, 255, 85),\n",
    "    (170, 255, 0),\n",
    "    (0, 85, 255),\n",
    "    (0, 255, 170),\n",
    "    (0, 0, 255),\n",
    "    (0, 255, 255),\n",
    "    (85, 0, 255),\n",
    "    (0, 170, 255),\n",
    ")\n",
    "\n",
    "default_skeleton = (\n",
    "    (15, 13),\n",
    "    (13, 11),\n",
    "    (16, 14),\n",
    "    (14, 12),\n",
    "    (11, 12),\n",
    "    (5, 11),\n",
    "    (6, 12),\n",
    "    (5, 6),\n",
    "    (5, 7),\n",
    "    (6, 8),\n",
    "    (7, 9),\n",
    "    (8, 10),\n",
    "    (1, 2),\n",
    "    (0, 1),\n",
    "    (0, 2),\n",
    "    (1, 3),\n",
    "    (2, 4),\n",
    "    (3, 5),\n",
    "    (4, 6),\n",
    ")\n",
    "\n",
    "\n",
    "def draw_poses(img, poses, point_score_threshold, skeleton=default_skeleton):\n",
    "    if poses.size == 0:\n",
    "        return img\n",
    "\n",
    "    img_limbs = np.copy(img)\n",
    "    for pose in poses:\n",
    "        points = pose[:, :2].astype(np.int32)\n",
    "        points_scores = pose[:, 2]\n",
    "        # Draw joints.\n",
    "        for i, (p, v) in enumerate(zip(points, points_scores)):\n",
    "            if v > point_score_threshold:\n",
    "                cv2.circle(img, tuple(p), 1, pose_colors[i], 2)\n",
    "        # Draw limbs.\n",
    "        for i, j in skeleton:\n",
    "            if points_scores[i] > point_score_threshold and points_scores[j] > point_score_threshold:\n",
    "                cv2.line(\n",
    "                    img_limbs,\n",
    "                    tuple(points[i]),\n",
    "                    tuple(points[j]),\n",
    "                    color=pose_colors[j],\n",
    "                    thickness=4,\n",
    "                )\n",
    "    cv2.addWeighted(img, 0.4, img_limbs, 0.6, 0, dst=img)\n",
    "    return img\n",
    "\n",
    "\n",
    "############detection\n",
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Read the network and corresponding weights from a file.\n",
    "model = core.read_model(model=converted_model_path)\n",
    "# Compile the model for CPU (you can choose manually CPU, GPU etc.)\n",
    "# or let the engine choose the best available device (AUTO).\n",
    "compiled_model = core.compile_model(model=model, device_name=device.value)\n",
    "\n",
    "# Get the input and output nodes.\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# Get the input size.\n",
    "height, width = list(input_layer.shape)[1:3]\n",
    "\n",
    "input_layer.any_name, output_layer.any_name\n",
    "\n",
    "###############\n",
    "\n",
    "##### pose estimation set up\n",
    "import ipywidgets as widgets\n",
    "\n",
    "core2 = ov.Core()\n",
    "\n",
    "device2 = widgets.Dropdown(\n",
    "    options=core2.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Initialize OpenVINO Runtime\n",
    "core2 = ov.Core()\n",
    "# Read the network from a file.\n",
    "model2 = core2.read_model(model_path2)\n",
    "# Let the AUTO device decide where to load the model (you can use CPU, GPU as well).\n",
    "compiled_model2 = core2.compile_model(model=model2, device_name=device2.value, config={\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "\n",
    "# Get the input and output names of nodes.\n",
    "input_layer2 = compiled_model2.input(0)\n",
    "output_layers2 = compiled_model2.outputs\n",
    "\n",
    "# Get the input size.\n",
    "height2, width2 = list(input_layer.shape)[2:]\n",
    "\n",
    "input_layer2.any_name, [o.any_name for o in output_layers2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1301b651-d4d1-46e5-ad50-59d7559e3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "classes = [\n",
    "    \"background\",\n",
    "    \"person\",\n",
    "    \"bicycle\",\n",
    "    \"car\",\n",
    "    \"motorcycle\",\n",
    "    \"airplane\",\n",
    "    \"bus\",\n",
    "    \"train\",\n",
    "    \"truck\",\n",
    "    \"boat\",\n",
    "    \"traffic light\",\n",
    "    \"fire hydrant\",\n",
    "    \"street sign\",\n",
    "    \"stop sign\",\n",
    "    \"parking meter\",\n",
    "    \"bench\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"sheep\",\n",
    "    \"cow\",\n",
    "    \"elephant\",\n",
    "    \"bear\",\n",
    "    \"zebra\",\n",
    "    \"giraffe\",\n",
    "    \"hat\",\n",
    "    \"backpack\",\n",
    "    \"umbrella\",\n",
    "    \"shoe\",\n",
    "    \"eye glasses\",\n",
    "    \"handbag\",\n",
    "    \"tie\",\n",
    "    \"suitcase\",\n",
    "    \"frisbee\",\n",
    "    \"skis\",\n",
    "    \"snowboard\",\n",
    "    \"sports ball\",\n",
    "    \"kite\",\n",
    "    \"baseball bat\",\n",
    "    \"baseball glove\",\n",
    "    \"skateboard\",\n",
    "    \"surfboard\",\n",
    "    \"tennis racket\",\n",
    "    \"bottle\",\n",
    "    \"plate\",\n",
    "    \"wine glass\",\n",
    "    \"cup\",\n",
    "    \"fork\",\n",
    "    \"knife\",\n",
    "    \"spoon\",\n",
    "    \"bowl\",\n",
    "    \"banana\",\n",
    "    \"apple\",\n",
    "    \"sandwich\",\n",
    "    \"orange\",\n",
    "    \"broccoli\",\n",
    "    \"carrot\",\n",
    "    \"hot dog\",\n",
    "    \"pizza\",\n",
    "    \"donut\",\n",
    "    \"cake\",\n",
    "    \"chair\",\n",
    "    \"couch\",\n",
    "    \"potted plant\",\n",
    "    \"bed\",\n",
    "    \"mirror\",\n",
    "    \"dining table\",\n",
    "    \"window\",\n",
    "    \"desk\",\n",
    "    \"toilet\",\n",
    "    \"door\",\n",
    "    \"tv\",\n",
    "    \"laptop\",\n",
    "    \"mouse\",\n",
    "    \"remote\",\n",
    "    \"keyboard\",\n",
    "    \"cell phone\",\n",
    "    \"microwave\",\n",
    "    \"oven\",\n",
    "    \"toaster\",\n",
    "    \"sink\",\n",
    "    \"refrigerator\",\n",
    "    \"blender\",\n",
    "    \"book\",\n",
    "    \"clock\",\n",
    "    \"vase\",\n",
    "    \"scissors\",\n",
    "    \"teddy bear\",\n",
    "    \"hair drier\",\n",
    "    \"toothbrush\",\n",
    "    \"hair brush\",\n",
    "]\n",
    "\n",
    "# Colors for the classes above (Rainbow Color Map).\n",
    "colors = cv2.applyColorMap(\n",
    "    src=np.arange(0, 255, 255 / len(classes), dtype=np.float32).astype(np.uint8),\n",
    "    colormap=cv2.COLORMAP_RAINBOW,\n",
    ").squeeze()\n",
    "\n",
    "\n",
    "def process_results(frame, results, thresh=0.6):\n",
    "    # The size of the original frame.\n",
    "    h, w = frame.shape[:2]\n",
    "    # The 'results' variable is a [1, 1, 100, 7] tensor.\n",
    "    results = results.squeeze()\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    scores = []\n",
    "    for _, label, score, xmin, ymin, xmax, ymax in results:\n",
    "        # Create a box with pixels coordinates from the box with normalized coordinates [0,1].\n",
    "        boxes.append(tuple(map(int, (xmin * w, ymin * h, (xmax - xmin) * w, (ymax - ymin) * h))))\n",
    "        labels.append(int(label))\n",
    "        scores.append(float(score))\n",
    "\n",
    "    # Apply non-maximum suppression to get rid of many overlapping entities.\n",
    "    # See https://paperswithcode.com/method/non-maximum-suppression\n",
    "    # This algorithm returns indices of objects to keep.\n",
    "    indices = cv2.dnn.NMSBoxes(bboxes=boxes, scores=scores, score_threshold=thresh, nms_threshold=0.6)\n",
    "\n",
    "    # If there are no boxes.\n",
    "    if len(indices) == 0:\n",
    "        return []\n",
    "\n",
    "    # Filter detected objects.\n",
    "    return [(labels[idx], scores[idx], boxes[idx]) for idx in indices.flatten()]\n",
    "\n",
    "\n",
    "def draw_boxes(frame, boxes):\n",
    "    for label, score, box in boxes:\n",
    "        # Choose color for the label.\n",
    "        if label == 1:\n",
    "            color = tuple(map(int, colors[label]))\n",
    "            # Draw a box.\n",
    "            x2 = box[0] + box[2]\n",
    "            y2 = box[1] + box[3]\n",
    "            cv2.rectangle(img=frame, pt1=box[:2], pt2=(x2, y2), color=color, thickness=3)\n",
    "        \n",
    "            # Draw a label name inside the box.\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=f\"{classes[label]} {score:.2f}\",\n",
    "                org=(box[0] + 10, box[1] + 30),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=frame.shape[1] / 1000,\n",
    "                color=color,\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "        return frame\n",
    "\n",
    "def crop_boxes(frame , boxes):\n",
    "    for label, score, box in boxes:\n",
    "        if label == 1:\n",
    "            x1 = box[0]\n",
    "            y1 = box[1]\n",
    "            x2 = box[0] + box[2]\n",
    "            y2 = box[1] + box[3]\n",
    "            crop_frame = frame[y1:y2, x1:x2]\n",
    "            return crop_frame ,height , width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9d66a56-3466-4ac0-aca8-cefd6ea7b31a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception from src/inference/src/cpp/infer_request.cpp:103:\nException from src/inference/src/cpp/infer_request.cpp:66:\nException from src/plugins/intel_cpu/src/infer_request.cpp:368:\nCan't set the input tensor with index: 0, because the model input (shape=[1,3,256,456]) and the tensor (shape=(1.3.300.3)) are incompatible\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m input_img2 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame2, (width2, height2), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_AREA)\n\u001b[1;32m     50\u001b[0m input_img2 \u001b[38;5;241m=\u001b[39m input_img2\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))[np\u001b[38;5;241m.\u001b[39mnewaxis, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m results2 \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_model2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_img2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m pafs2 \u001b[38;5;241m=\u001b[39m results2[pafs_output_key]\n\u001b[1;32m     53\u001b[0m heatmaps2 \u001b[38;5;241m=\u001b[39m results2[heatmaps_output_key]\n",
      "File \u001b[0;32m~/app/app_env/lib/python3.10/site-packages/openvino/runtime/ie_api.py:388\u001b[0m, in \u001b[0;36mCompiledModel.__call__\u001b[0;34m(self, inputs, share_inputs, share_outputs, decode_strings)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_request \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_infer_request()\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshare_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshare_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_strings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/app/app_env/lib/python3.10/site-packages/openvino/runtime/ie_api.py:132\u001b[0m, in \u001b[0;36mInferRequest.infer\u001b[0;34m(self, inputs, share_inputs, share_outputs, decode_strings)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     57\u001b[0m     inputs: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     decode_strings: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OVDict:\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Infers specified input(s) in synchronous mode.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    Blocks all methods of InferRequest while request is running.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    :rtype: OVDict\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OVDict(\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_data_dispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_shared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_strings\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception from src/inference/src/cpp/infer_request.cpp:103:\nException from src/inference/src/cpp/infer_request.cpp:66:\nException from src/plugins/intel_cpu/src/infer_request.cpp:368:\nCan't set the input tensor with index: 0, because the model input (shape=[1,3,256,456]) and the tensor (shape=(1.3.300.3)) are incompatible\n\n\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# 저장한 동영상 파일 열기\n",
    "cap = cv2.VideoCapture('./resource/video/2.mp4')\n",
    "\n",
    "# 동영상 파일 열기 확인\n",
    "if not cap.isOpened():\n",
    "    print(\"동영상 파일을 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 동영상 파일의 프레임 너비와 높이 가져오기\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "pafs_output_key = compiled_model2.output(\"Mconv7_stage2_L1\")\n",
    "heatmaps_output_key = compiled_model2.output(\"Mconv7_stage2_L2\")\n",
    "actions = ['0']\n",
    "seq_length = 90\n",
    "secs_for_action = 45\n",
    "created_time = int(time.time())\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "data = []\n",
    "\n",
    "# 동영상 재생\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    scale = 1280 / max(frame.shape)\n",
    "    if scale < 1:\n",
    "        frame = cv2.resize(\n",
    "            src=frame,\n",
    "            dsize=None,\n",
    "            fx=scale,\n",
    "            fy=scale,\n",
    "            interpolation=cv2.INTER_AREA,\n",
    "        )\n",
    "    \n",
    "    # Resize the image and change dims to fit neural network input.\n",
    "    input_img = cv2.resize(src=frame, dsize=(width, height), interpolation=cv2.INTER_AREA)\n",
    "    # Create a batch of images (size = 1).\n",
    "    input_img = input_img[np.newaxis, ...]\n",
    "    results = compiled_model([input_img])[output_layer]\n",
    "    boxes = process_results(frame=frame, results=results)\n",
    "    frame = draw_boxes(frame=frame, boxes=boxes)\n",
    "    frame2,pose_height,pose_width = crop_boxes(frame=frame,boxes=boxes)\n",
    "\n",
    "    frame2 = np.zeros((pose_height, pose_width, 3), dtype=np.uint8)\n",
    "    input_img2 = cv2.resize(frame2, (width2, height2), interpolation=cv2.INTER_AREA)\n",
    "    input_img2 = input_img2.transpose((2, 0, 1))[np.newaxis, ...]\n",
    "    results2 = compiled_model2([input_img2])\n",
    "    pafs2 = results2[pafs_output_key]\n",
    "    heatmaps2 = results2[heatmaps_output_key]\n",
    "    poses, scores = process_results(frame2, pafs2, heatmaps2)\n",
    "\n",
    "    if poses is not None:\n",
    "        for pose in poses:\n",
    "            points = pose[:,:2]           \n",
    "            joint = np.zeros((18, 2))\n",
    "            for j, lm in enumerate(pose):\n",
    "                joint[j] = [points[j,0], points[j,1]]\n",
    "    \n",
    "            # Compute angles between joints\n",
    "            v1 = joint[[ 1, 1, 1, 5, 5, 6, 6, 7, 8,12,11,13,12,14], :] # Parent joint\n",
    "            v2 = joint[[ 2, 5, 6, 7,11,12, 8, 9,10,11,13,15,14,16], :] # Child joint\n",
    "            v = v2 - v1 # [18, 2]\n",
    "            # Normalize v\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "    \n",
    "            # Get angle using arcos of dot product\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "            v[[ 0, 0, 1, 1, 3, 3, 2, 2, 6, 6, 4, 4, 5, 5, 9, 9,10,12],:],\n",
    "            v[[ 1, 2, 3, 4, 7, 4, 5, 6, 5, 8, 9,10, 9,12,10,12,11,13],:])) #[18,2]\n",
    "            angle = np.degrees(angle) # Convert radian to degree\n",
    "            angle_label = np.array([angle], dtype=np.float32)\n",
    "            angle_label = np.append(angle_label, 0)\n",
    "            d = np.concatenate([joint.flatten(), angle_label])\n",
    "            print(f\"points = {d.shape}\")\n",
    "            data.append(d)\n",
    "    \n",
    "    cv2.imshow('Saved Video', frame)\n",
    "    # Create sequence data\n",
    "    full_seq_data = []\n",
    "    for seq in range(len(data) - seq_length):\n",
    "        full_seq_data.append(data[seq:seq + seq_length])\n",
    "\n",
    "    full_seq_data = np.array(full_seq_data)\n",
    "    print(action, full_seq_data.shape)\n",
    "    np.save(os.path.join('dataset', f'seq_{action}'), full_seq_data)\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(40) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e7839-5270-4435-aac0-e59409437f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
